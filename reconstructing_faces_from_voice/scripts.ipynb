{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, exists\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as tranforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mfcc import MFCC\n",
    "from network import get_network\n",
    "from utils import voice2face, npy2face, getNpy, npy2face_e2e\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from network import VoiceEmbedNet, Generator, Voice2Face\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORKS_PARAMETERS = {\n",
    "    # VOICE EMBEDDING NETWORK (e)\n",
    "    'eg': {\n",
    "        'network': Voice2Face,\n",
    "        'input_channel': 64,\n",
    "        'channels': [[256, 384, 576, 864], 64, [1024, 512, 256, 128, 64]],\n",
    "        'output_channel': 3, # the embedding dimension\n",
    "        'model_path': 'training_models/voice2face_10.pth',\n",
    "    },\n",
    "    'e': {\n",
    "        'network': VoiceEmbedNet,\n",
    "        'input_channel': 64,\n",
    "        'channels': [256, 384, 576, 864],\n",
    "        'output_channel': 64, # the embedding dimension\n",
    "        'model_path': 'pretrained_models/voice_embedding.pth',\n",
    "    },\n",
    "    # GENERATOR (g)\n",
    "    'g': {\n",
    "        'network': Generator,\n",
    "        'input_channel': 64,\n",
    "        'channels': [1024, 512, 256, 128, 64], # channels for deconvolutional layers\n",
    "        'output_channel': 3, # images with RGB channels\n",
    "        # 'model_path': 'training_models/generator.pth',\n",
    "        'model_path': 'pretrained_models/generator_100.pth',\n",
    "    },\n",
    "    'GPU': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_net, _ = get_network('e', NETWORKS_PARAMETERS, train=False)\n",
    "g_net, _ = get_network('g', NETWORKS_PARAMETERS, train=False)\n",
    "eg_net, _ = get_network('eg', NETWORKS_PARAMETERS, train=False)\n",
    "# ids_list = [10190, 10167, 10182, 10306, 10498]\n",
    "\n",
    "# old_list = [10090, 10086, 10116, 10303, 10203, 10242, 10096, 10088]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tranforms.Resize(160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metafile(meta_file):\n",
    "    with open(meta_file, 'r') as f:\n",
    "        lines = f.readlines()[1:]\n",
    "    celeb_ids = {}\n",
    "    for line in lines:\n",
    "        ID, name, gender, nation, dev = line.rstrip().split('\\t')\n",
    "        celeb_ids[ID] = (name, gender, nation, dev)\n",
    "    return celeb_ids\n",
    "\n",
    "def load_face(face_path):\n",
    "    face_data = Image.open(face_path).convert('RGB').resize([64, 64])\n",
    "    face_data = np.transpose(np.array(face_data), (2, 0, 1))\n",
    "    face_data = ((face_data - 127.5) / 127.5).astype('float32')\n",
    "    return face_data\n",
    "\n",
    "\n",
    "def load_test_face(face_path):\n",
    "    face_data = Image.open(face_path).convert('RGB').resize([160, 160])\n",
    "    face_data = np.transpose(np.array(face_data), (2, 0, 1))\n",
    "    face_data = ((face_data - 127.5) / 127.5).astype('float32')\n",
    "    return face_data\n",
    "\n",
    "celeb_map = parse_metafile('data/vox1_meta.csv')\n",
    "cropped_face_root = 'data/cropped_faces/'\n",
    "face_root = 'data/VGG_ALL_FRONTAL/'\n",
    "\n",
    "def get_face(celeb_id, path):\n",
    "    name = celeb_map['id'+(str(celeb_id))][0]\n",
    "    face_folder = path + name\n",
    "    if not exists(face_folder):\n",
    "        return None\n",
    "    onlyfiles = [f for f in listdir(face_folder) if f.endswith('jpg') and isfile(join(face_folder, f))]\n",
    "    face = load_test_face(join(face_folder, onlyfiles[12]))\n",
    "    return torch.Tensor(face).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 136/1251 [00:22<02:58,  6.24it/s]"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "targets = []\n",
    "id_list = []\n",
    "for ids in tqdm(celeb_map.keys(), total=len(celeb_map.keys())):\n",
    "   \n",
    "    name, gender, nation, dev = celeb_map[ids]\n",
    "    id_num = int(ids[2:])\n",
    "    target_path = get_face(id_num, face_root)\n",
    "    if target_path is None:\n",
    "        continue\n",
    "    id_list.append(ids)\n",
    "    face_target = transform(target_path).reshape((1, 3, 160, 160))\n",
    "    arr = getNpy(id_num)\n",
    "    face_image = npy2face_e2e(eg_net, arr, NETWORKS_PARAMETERS['GPU'])\n",
    "    face = transform(face_image)\n",
    "    emb = model(face)\n",
    "    \n",
    "    emb_target = model(face_target)\n",
    "\n",
    "    targets.append(emb_target.detach().cpu())\n",
    "    embs.append(emb.detach().cpu())\n",
    "    torch.cuda.empty_cache()\n",
    "    del face_target\n",
    "    del face\n",
    "    del face_image\n",
    "    del emb\n",
    "    del emb_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct rate:  0.8628624783246632\n",
      "average distance aligned:  1.1657864985174062\n",
      "average distance misaligned:  1.3968267040988025\n"
     ]
    }
   ],
   "source": [
    "n = len(embs)\n",
    "sims = [(e1 - e2).norm().item() for (e1, e2) in zip(targets, embs)]\n",
    "dists = [[(e1 - e2).norm().item() for e2 in embs] for e1 in targets]\n",
    "df = pd.DataFrame(dists, columns=id_list, index=id_list)\n",
    "correct_rate = (df>sims).sum().sum()/(n*(n - 1))\n",
    "print('correct rate: ', correct_rate)\n",
    "print('average distance aligned: ', np.mean(sims))\n",
    "print('average distance misaligned: ', (df.sum().sum() - np.sum(sims))/(n*(n-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(embs)\n",
    "sims1 = [(e1 - e2).norm().item() for (e1, e2) in zip(targets, embs)]\n",
    "dists1 = [[(e1 - e2).norm().item() for e2 in embs] for e1 in targets]\n",
    "df1 = pd.DataFrame(dists1, columns=id_list, index=id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stratify:\n",
    "id_stra = []\n",
    "test_letter = set(string.ascii_uppercase[:5])\n",
    "sims_stra = []\n",
    "for i in range(len(id_list)):\n",
    "    ids = id_list[i]\n",
    "    name, gender, nation, dev = celeb_map[ids]\n",
    "    if name[0] in test_letter and gender != 'm':\n",
    "        sims_stra.append(sims[i])\n",
    "        id_stra.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct rate:  0.5100413046574562\n",
      "average distance aligned:  1.3650550646942203\n",
      "average distance misaligned:  1.3723751706897729\n"
     ]
    }
   ],
   "source": [
    "df_stra = df.loc[id_stra, id_stra]\n",
    "s = len(sims_stra)\n",
    "correct_rate = (df_stra>sims_stra).sum().sum()/(s*(s - 1))\n",
    "print('correct rate: ', correct_rate)\n",
    "print('average distance aligned: ', np.mean(sims_stra))\n",
    "print('average distance misaligned: ', (df_stra.sum().sum() - np.sum(sims_stra))/(s*(s-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct rate:  0.610531561461794\n",
    "average distance aligned:  1.3470759758125508\n",
    "average distance misaligned:  1.3938977525948422\n",
    "\n",
    "male:\n",
    "correct rate:  0.5815372472831036\n",
    "average distance aligned:  1.335320417697613\n",
    "average distance misaligned:  1.3709077735910808\n",
    "\n",
    "correct rate:  0.5100413046574562\n",
    "average distance aligned:  1.3650550646942203\n",
    "average distance misaligned:  1.3723751706897729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sims1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3989/1606039854.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mceleb_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_letter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgender\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msims_stra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msims1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mid_stra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sims1' is not defined"
     ]
    }
   ],
   "source": [
    "id_stra = []\n",
    "test_letter = set(string.ascii_uppercase[:5])\n",
    "sims_stra = []\n",
    "for i in range(len(id_list)):\n",
    "    ids = id_list[i]\n",
    "    name, gender, nation, dev = celeb_map[ids]\n",
    "    if name[0] in test_letter and gender != 'm':\n",
    "        sims_stra.append(sims1[i])\n",
    "        id_stra.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct rate:  0.6424473975636766\n",
      "average distance aligned:  1.3355389045322457\n",
      "average distance misaligned:  1.393775611909917\n"
     ]
    }
   ],
   "source": [
    "df_stra = df1.loc[id_stra, id_stra]\n",
    "s = len(sims_stra)\n",
    "correct_rate = (df_stra>sims_stra).sum().sum()/(s*(s - 1))\n",
    "print('correct rate: ', correct_rate)\n",
    "print('average distance aligned: ', np.mean(sims_stra))\n",
    "print('average distance misaligned: ', (df_stra.sum().sum() - np.sum(sims_stra))/(s*(s-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sims_stra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "119 301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5980948419585863\n"
     ]
    }
   ],
   "source": [
    "fm = 119/301\n",
    "mm = 1 - fm\n",
    "mt = 0.615111407929087\n",
    "ft = 0.572069505768409\n",
    "print(fm*ft + mm*mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9dc663c12342db218673069099c5f4d734584a05de3b02b725edaf289dd9612"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('pytorch_latest_p37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
